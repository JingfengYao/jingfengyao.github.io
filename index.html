<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Jingfeng Yao (姚劲枫) </title>
    <link rel="stylesheet" href="./style.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/all.min.css">
    <link rel="stylesheet" href="https://huggingface.co/front/assets/huggingface_logo.svg" crossorigin="anonymous">
</head>
<body>
    <!-- Header Section -->
    <header class="header-section">
        <h1>Jingfeng Yao (姚劲枫)</h1>
    </header>

    <!-- About Me Section -->
    <section id="about" class="content-section">
        <div class="profile-container">
            <div class="profile-photo">
                <img src="images/jingfeng.jpg" alt="Jingfeng Yao">
            </div>
            <div class="profile-info">
                <p>
                    I'm a third-year Ph.D. student at Huazhong University of Science and Technology (HUST), supervised by Prof. Xinggang Wang, and <strong>expected to graduate in 2027</strong>. My previous research includes image matting and representation learning. Currently, my research interests focus on <strong>generative models</strong>.
                    <ul>
                        <li>MiniMax (2024.03-present): Algorithm Intern, contributing to Hailuo series video generative models</li>
                        <li>Xiaobing AI (2022.01-2022.12): Algorithm Intern, conducted research on image matting</li>
                    </ul>
                </p>
                <div class="contact-links">
                    <a href="mailto:jfyao@hust.edu.cn"><i class="fas fa-envelope"></i> Email</a>
                    <a href="https://github.com/JingfengYao"><i class="fab fa-github"></i> GitHub</a>
                    <a href="https://scholar.google.com/citations?user=4qc1qJ0AAAAJ&hl=en"><i class="fas fa-graduation-cap"></i> Google Scholar</a>
                </div>
            </div>
        </div>
    </section>

    <!-- Publications Section -->
    <section id="publications" class="content-section">
        <h2>Publications</h2>
        <h3>Selected Publications</h3>
        <div class="publication-list">
            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/vavae.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>Reconstruction vs. Generation: Taming Optimization Dilemma in Latent Diffusion Models</strong>
                    </div>
                    <div class="pub-authors">
                        <strong>Jingfeng Yao</strong>, Bin Yang, Xinggang Wang
                    </div>
                    <div class="pub-venue" style="color: #000000; font-style: italic;">
                        CVPR 2025 <strong>Oral</strong> (CCF-A)
                    </div>
                    <div class="pub-highlight" style="color: #e65100;">
                        <i class="fas fa-star" style="color: #ffd700;"></i> Accepted with FULL score by CVPR.<br>
                        <i class="fas fa-trophy" style="color: #ffd700;"></i> Rank 1st in <a href="https://paperswithcode.com/sota/image-generation-on-imagenet-256x256">ImageNet 256×256 Generation</a> with FID=1.35 (During 2025.01-02)<br>
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2501.01423"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/hustvl/LightningDiT?tab=readme-ov-file"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>
            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/vitmatte.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>ViTMatte: Boosting Image Matting with Pretrained Plain Vision Transformers</strong>
                    </div>
                    <div class="pub-authors">
                        <strong>Jingfeng Yao</strong>, Xinggang Wang, Shusheng Yang, Baoyuan Wang
                    </div>
                    <div class="pub-venue" style="color: #000000; font-style: italic;">
                        Information Fusion (IF=18.6 at acceptance), 2023
                    </div>
                    <div class="pub-highlight" style="color: #e65100;">
                        <i class="fas fa-star"></i> Integrated into <a href="https://huggingface.co/docs/transformers/model_doc/vitmatte"><img src="https://huggingface.co/front/assets/huggingface_logo.svg" alt="Hugging Face" style="height: 1em; vertical-align: middle;"> Hugging Face Transformers</a> as a standard matting method.<br>
                        <i class="fas fa-star"></i> Integrated into the professional tool Nuke, with model weights <a href="https://huggingface.co/hustvl/vitmatte-small-composition-1k">downloaded over 46 million times</a>.
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2305.15272"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/hustvl/ViTMatte?tab=readme-ov-file"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>
        </div>

        <h3>Other Publications (First author or co-first author only)</h3>
        <div class="publication-list">

            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/vtp.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>Towards Scalable Pre-training of Visual Tokenizers for Generation</strong>
                    </div>
                    <div class="pub-authors">
                        <strong>Jingfeng Yao</strong>, Yuda Song, Yucong Zhou, Xinggang Wang
                    </div>
                    <div class="pub-venue" style="color: #666666; font-style: italic;">
                        arXiv preprint, 2025
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2512.13687"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/MiniMax-AI/VTP"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/diffusionvl.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</strong>
                    </div>
                    <div class="pub-authors">
                        Lunbin Zeng*, <strong>Jingfeng Yao*</strong>, Bencheng Liao, Hongyuan Tao, Wenyu Liu, Xinggang Wang (* denotes equal contribution)
                    </div>
                    <div class="pub-venue" style="color: #666666; font-style: italic;">
                        arXiv preprint, 2025
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2512.15713"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/hustvl/DiffusionVL"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/unix.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>UniX: Unifying Autoregression and Diffusion for Chest X-Ray Understanding and Generation</strong>
                    </div>
                    <div class="pub-authors">
                        Ruiheng Zhang*, <strong>Jingfeng Yao*</strong>, Huangxuan Zhao*, Hao Yan, Xiao He, Lei Chen, Zhou Wei, Yong Luo, Zengmao Wang, Lefei Zhang, Dacheng Tao, Bo Du (* denotes equal contribution)
                    </div>
                    <div class="pub-venue" style="color: #666666; font-style: italic;">
                        arXiv preprint, 2026
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2601.11522"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/ZrH42/UniX"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/turbo_vaed.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>Turbo-VAED: Fast and Stable Transfer of Video-VAEs to Mobile Devices</strong>
                    </div>
                    <div class="pub-authors">
                        Ya Zou*, <strong>Jingfeng Yao*</strong>, Siyuan Yu, Shuai Zhang, Wenyu Liu, Xinggang Wang
                        (* denotes equal contribution)
                    </div>
                    <div class="pub-venue" style="color: #000000; font-style: italic;">
                        AAAI 2026 (CCF-A)
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2508.09136"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/hustvl/Turbo-VAED"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/fasterdit.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>FasterDiT: Towards Faster Diffusion Transformers Training without Architecture Modification</strong>
                    </div>
                    <div class="pub-authors">
                        <strong>Jingfeng Yao</strong>, Cheng Wang, Wenyu Liu, Xinggang Wang
                    </div>
                    <div class="pub-venue" style="color: #000000; font-style: italic;">
                        NeurIPS 2024 (CCF-A)
                    </div>
                    <!-- <div class="pub-highlight" style="color: #e65100;">
                        <i class="fas fa-search"></i> Analytical theory that observes diffusion training process with Probability Density Function of SNR.
                    </div> -->
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2410.10356"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/hustvl/LightningDiT?tab=readme-ov-file"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/evax.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>EVA-X: A Foundation Model for General Chest X-ray Analysis with Self-supervised Learning</strong>
                    </div>
                    <div class="pub-authors">
                        <strong>Jingfeng Yao</strong>, Xinggang Wang, Yuehao Song, Huangxuan Zhao, Jun Ma, Yajie Chen, Wenyu Liu, Bo Wang
                    </div>
                    <div class="pub-venue" style="color: #000000; font-style: italic;">
                        npj Digital Medicine (<strong>Nature</strong> Partner Journals, IF=15.1 at acceptance)
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2405.05237"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/hustvl/EVA-X"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>

            
            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/lkcell.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>LKCell: Efficient Cell Nuclei Instance Segmentation with Large Convolution Kernels</strong>
                    </div>
                    <div class="pub-authors">
                        Ziwei Cui*, <strong>Jingfeng Yao</strong>*, Lunbin Zeng, Juan Yang, Wenyu Liu, Xinggang Wang
                        (* denotes equal contribution)
                    </div>
                    <div class="pub-venue" style="color: #666666; font-style: italic;">
                        arXiv preprint, 2024
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2407.18054"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/hustvl/LKCell"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>

            <div class="publication-item">
                <div class="pub-thumbnail">
                    <img src="images/matteanything.png" alt="Paper thumbnail">
                </div>
                <div class="pub-content">
                    <div class="pub-title">
                        <strong>Matte Anything! Interactive Natural Image Matting with Segment Anything Models</strong>
                    </div>
                    <div class="pub-authors">
                        <strong>Jingfeng Yao</strong>, Xinggang Wang, Lang Ye, Wenyu Liu
                    </div>
                    <div class="pub-venue" style="color: #000000; font-style: italic;">
                        Image and Vision Computing (CCF-C), 2024
                    </div>
                    <div class="pub-links">
                        <a href="https://arxiv.org/abs/2306.04121"><i class="fas fa-file-pdf"></i> arXiv</a>
                        <a href="https://github.com/hustvl/Matte-Anything?tab=readme-ov-file"><i class="fab fa-github"></i> code</a>
                    </div>
                </div>
            </div>

        </div>
    </section>

    <!-- Awards Section -->
    <section id="awards" class="content-section">
        <h2>Awards</h2>
        <ul class="awards-list">
            <li>China National Scholarship 2024<br>
                国家奖学金
                <div class="award-highlight" style="color: #666666;">
                    <i class="fas fa-award" style="color: #ffd700;"></i> The most prestigious honor for university students in China, awarded to only <strong>0.2%</strong> of candidates nationwide.
                </div>
            </li>
            <li>Gold Award in China College Students' 'Internet+' Innovation and 
                Entrepreneurship Competition 2022<br>
                中国大学生"互联网+"创新创业大赛 国赛金奖
                <div class="award-highlight" style="color: #666666;">
                    <i class="fas fa-medal" style="color: #ffd700;"></i> The largest global innovation event, with a Gold Award success rate of only <strong>0.009%</strong> (about 300 out of 3.4 million projects)
                </div>
            </li>
        </ul>
    </section>

    <!-- Fun Facts Section -->
    <section id="fun-facts" class="content-section">
        <h2>Fun Facts</h2>
        <div class="fun-fact-list">
            <div class="fun-fact-item">
                <div class="fact-thumbnail">
                    <img src="images/volunteer.jpg" alt="Volunteer teaching">
                </div>
                <div class="fact-content">
                    <p>I spent <strong>one year as a volunteer teacher</strong> in Lincang, Yunnan Province, representing Huazhong University of Science and Technology. I taught history at Linxiang No.1 Middle School.</p>
                    <div class="fact-links">
                        <a href="https://www.thepaper.cn/newsDetail_forward_9929789" target="_blank">
                            <i class="fas fa-newspaper"></i> News Coverage
                        </a>
                    </div>
                </div>
            </div>
        </div>
    </section>

    <script src="scripts.js"></script>
</body>
</html>
